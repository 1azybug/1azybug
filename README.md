### Hi there ğŸ‘‹

<!--
**1azybug/1azybug** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ‘¯ Iâ€™m looking to collaborate on ...
- ğŸ¤” Iâ€™m looking for help with ...
- ğŸ’¬ Ask me about ...
- ğŸ“« How to reach me: ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->


I'm Runsong Zhao èµµæ¶¦æ¾, an AI major student studying for creating AI from ACGN world!

## Now working on:
> - implementing the experiment of "Attention Is All You Need"
> - reading paper about LLMs
> - Improving the design of LLM in terms of architecture, data and training strategy



## Future work:
> - to implement gpt2(117M)
> - to read paper about reinforcement learning, Memory network and Multimodal 

## Have read:

|Survey|
|:---------:|
|Open-domain Dialogue Generation:What We Can Do, Cannot Do, And Should Do Next|
|(2020)Towards Unified Dialogue System Evaluation|
|2022-A Survey for In-context Learning|
|Emergent Abilities of Large Language Models|
| **Dialogue system** |
|(2015)Neural Responding Machine for Short-Text Conversation|
|(2015)A Neural Conversational Model|
|(2018)Extending Neural Generative Conversational Model using External Knowledge Sources|
|(2016)A Persona-Based Neural Conversation Model|
|(2018b)Personalizing Dialogue Agents I have a dog, do you have pets too|
|(2018b) \[Note\] Personalizing Dialogue Agents I have a dog, do you have pets too|
|(2018)An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation|
|(2020)Enhancing Dialogue Generation via Multi-Level Contrastive Learning|
|(Zhang 2020a)Modeling Topical Relevance for Multi-Turn Dialogue Generation|
|(Ghazvininejad 2017)A knowledge-grounded neural conversation model|
|(2022 Ju)Learning to Improve Persona Consistency in Multi-party Dialogue|
| **LLMs** |
|(2018 Radford)Improving Language Understanding by Generative Pre-Training|
|(GPT2)language models are unsupervised multitask learners|
|GPT3 Language Models are Few-Shot Learners|
|(instructGPT)Training language models to follow instructions with human feedback|
|**Reinforcement learning**|
|(PPO)Proximal Policy Optimization Algorithms|
|**other**|
|(2002)BLEU a Method for Automatic Evaluation of Machine Translation|
|(2019 Feng)Answer-guided and Semantic Coherent Question Generation|
|(2020 Feng)A Co-Attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness|
|(transformer)Attention Is All You Need|
|(latent diffusion)High-Resolution Image Synthesis with Latent Diffusion Models|

## ä¸ªäººç»å†

é«˜ä¸­æ¯•ä¸šæ—¶æƒ³è¦åšå‡ºè®¸å¤šå¹»æƒ³ä½œå“ä¸­çš„AIï¼ˆä¾‹å¦‚[Atri](https://baike.baidu.com/item/%E4%BA%9A%E6%89%98%E8%8E%89/50041942?fromModule=lemma-qiyi_sense-lemma&fromtitle=ATRI&fromid=50218712)ï¼‰ï¼Œäºæ˜¯ç¬¬ä¸€å¿—æ„¿æŠ¥äº†ä¸œåŒ—å¤§å­¦çš„äººå·¥æ™ºèƒ½ä¸“ä¸šã€‚æˆ‘å½“æ—¶çš„æƒ³æ³•æ˜¯åªè¦AIå‡ºç°å°±å¥½ï¼Œä¸æ˜¯æˆ‘åˆ›é€ çš„ä¹Ÿæ²¡å…³ç³»ã€‚æˆ‘å½“æ—¶åªäº†è§£åˆ°ç¥ç»ç½‘ç»œã€å¼ºåŒ–å­¦ä¹ ç­‰åè¯ï¼Œä¹Ÿä¸çŸ¥é“ä»–ä»¬æ˜¯ä»€ä¹ˆæ„æ€ã€‚

å¤§ä¸€æ—¶ï¼Œæˆ‘çœ‹äº†å¯¼è®ºï¼Œä½†åªäº†è§£åˆ°â€œç¬¦å·ä¸»ä¹‰â€ï¼Œâ€œè¿æ¥ä¸»ä¹‰â€ï¼Œâ€œAGIâ€ç­‰æ³›æ³›è€Œè°ˆçš„æ¦‚å¿µå’Œå†å²ï¼Œè€Œå¯¹å…·ä½“çš„æŠ€æœ¯ä¸€æ— æ‰€çŸ¥ã€‚äºæ˜¯æˆ‘å»æ‰¾ç¥ç»ç½‘ç»œçš„èµ„æ–™å»å­¦ä¹ ï¼Œå¾ˆé—æ†¾ï¼Œå› ä¸ºæ•°å­¦çŸ¥è¯†çš„ç¼ºä¹ï¼Œå­¦ä¹ å¹¶ä¸é¡ºåˆ©ï¼Œä½†è¿˜æ˜¯èƒ½ç†è§£åˆ°ç¥ç»ç½‘ç»œæ˜¯é€šè¿‡â€œæƒé‡â€å»å½±å“å†³ç­–çš„ã€‚åˆ°æ­¤ï¼Œæˆ‘çš„AIå­¦ä¹ ä¹‹è·¯å¼€å§‹åœæ»ï¼Œè¿™æ®µæ—¶é—´æˆ‘å‚åŠ äº†å­¦æ ¡çš„ACMé˜Ÿã€‚

å¤§äºŒä¸Šï¼Œæˆ‘é€šè¿‡æ ¡å†…æ¸ é“å‚åŠ äº†ç™¾åº¦çš„AIåŸ¹è®­ç­ï¼Œäº†è§£åˆ°CVå’ŒNLPä¸¤ä¸ªæ–¹å‘ã€‚æˆ‘è®¤ä¸ºNLPæ˜¯æ›´è§¦åŠçµé­‚çš„éƒ¨åˆ†ï¼Œæ‰€ä»¥æˆ‘é€‰æ‹©äº†NLPã€‚

å¤§äºŒä¸‹ï¼Œæœ€ä¼˜åŒ–å­¦å®Œåï¼Œæˆ‘å­¦ä¹ æ·±åº¦å­¦ä¹ çš„æŠ€æœ¯æ²¡æœ‰å¤ªå¤§çš„éšœç¢äº†ï¼Œä½†æ˜¯çŸ¥è¯†é¢ä¸å¤Ÿå¹¿ã€‚å½“æ—¶æˆ‘è®¤ä¸ºæ‰¾åˆ°äº†æ­£ç¡®çš„æ–¹å‘ï¼Œæ‰€ä»¥é€‰æ‹©é€€å‡ºACMé˜Ÿï¼ˆå¤§ä¸‰æ‰èƒ½å‚åŠ ACM-ICPCï¼Œè™½ç„¶å¾ˆé—æ†¾ï¼Œä½†æˆ‘ä¸èƒ½æŠŠæ—¶é—´èŠ±åœ¨è¿™é‡Œ äº†ï¼‰ï¼ŒæŠŠé‡å¿ƒæ”¾åˆ°å­¦ä¹ NLPä¸Šã€‚æˆ‘åœ¨æš‘å‡å®Œæˆäº†ç™¾åº¦AIåŸ¹è®­ç­çš„å¤§éƒ¨åˆ†é¡¹ç›®ï¼ŒåŒæ—¶ç¡®è®¤äº†NLPå„ç§æŠ€æœ¯ä¸­â€œå¼€æ”¾åŸŸå¯¹è¯ç³»ç»Ÿâ€ä¸æˆ‘æƒ³è¦çš„AIæœ€åƒå¹¶å¼€å§‹é˜…è¯»ç›¸å…³çš„è®ºæ–‡ã€‚åŒæ—¶ä»è¿™ä¸ªæ—¶é—´ç‚¹å¼€å§‹åˆ°ç°åœ¨ï¼Œæˆ‘å¼€å§‹æ¶Œç°å‡ºå„ç§å…³äºAIçš„ideaï¼Œæˆ‘æŠŠæ¯ä¸ªideaéƒ½è®°åœ¨äº†ä¾¿ç­¾é‡Œï¼ˆå¾€åå­¦ä¹ è¯¾ç¨‹å’Œé˜…è¯»è®ºæ–‡ï¼Œæˆ‘å‘ç°è®¸å¤šideaå·²ç»è¢«è¯å®æœ‰æ•ˆï¼Œè€Œæœ‰äº›è¿˜æ²¡ç›¸åŒçš„æƒ³æ³•ï¼‰ã€‚

åŒæ—¶ä¹Ÿæ˜¯åœ¨å¤§äºŒæš‘å‡å¼€å§‹,å„ç§AIæŠ€æœ¯å¼€å§‹åº”ç”¨åˆ°äºŒæ¬¡å…ƒä¸­ã€‚é¦–å…ˆæ˜¯è¯­è¨€åˆæˆï¼šTacotron2ã€TTSå’ŒVITS(8æœˆ)ï¼Œç„¶åæ˜¯å¯¹è¯ç³»ç»Ÿï¼šå½©äº‘å°æ¢¦2.0ï¼ˆ8æœˆï¼‰ï¼Œæœ€åæ˜¯å›¾åƒåˆæˆï¼šnovelai(8æœˆå‡º,10æœˆç«)

å¤§ä¸‰ä¸Šï¼Œä¸“ä¸šè¯¾å¼€å§‹ï¼Œåº•å±‚çš„åŸç†å’Œé€»è¾‘ä¹Ÿææ˜ç™½äº†ï¼ŒåŒæ—¶æ¯æ¬¡ä¸Šè¯¾éƒ½ä¼šå¶å°”è¿¸å‡ºä¸€äº›ideaã€‚åŒæ—¶è¿«äºå‡å­¦çš„å‹åŠ›å¼€å§‹å·ç»©ç‚¹ã€‚å¯’å‡ï¼Œè”ç³»ä¸Šäº†æœ¬æ ¡çš„å†¯æ—¶è€å¸ˆï¼Œå†¯è€å¸ˆäº†è§£äº†æˆ‘çš„æƒ…å†µï¼Œè®©æˆ‘å»è°ƒç ”ç°åœ¨çš„å¯¹è¯ç³»ç»Ÿã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„è½¬æŠ˜ç‚¹ã€‚æˆ‘ä½“éªŒäº†å½“æ—¶ï¼ˆä¸€æœˆä»½ï¼‰å¤§éƒ¨åˆ†æœ‰æ½œåŠ›çš„å¯¹è¯ç³»ç»Ÿï¼Œå¾ˆé—æ†¾ï¼Œå¤§å¤šæ•°éƒ½æ˜¯äººå·¥æ™ºéšœã€‚ä¸è¿‡è¾›è¿çš„æ˜¯ï¼Œæœ‰ä¸‰ä¸ªäº§å“ä¸æ˜¯æ™ºéšœã€‚ä»–ä»¬åˆ†åˆ«æ˜¯ï¼šChatGPTã€character AIå’ŒGlow(å½“æ—¶Neuro-samaè¿˜åœ¨å°ç¦ï¼ŒMossè¿˜æ²¡å‡ºæ¥,ä¸ä½œè¯„ä»·)ã€‚å…¶ä¸­Glowæ˜¯å›½äº§çš„ï¼Œå½“æ—¶æˆ‘å¿«å¯¹å›½å†…çš„äº§å“å¤±æœ›äº†ï¼Œæœ€åæ‰¾åˆ°Glowè®©æˆ‘æ¢å¤äº†ä¿¡å¿ƒã€‚ç”±äºä¸‰ä¸ªäº§å“ä¸­ï¼Œåªæœ‰ChatGPTé€éœ²äº†æŠ€æœ¯ç»†èŠ‚ï¼Œæ‰€ä»¥æˆ‘å»é˜…è¯»äº†chatGPTçš„ç›¸å…³è®ºæ–‡ã€‚ä»æ­¤ï¼Œæˆ‘ä»å¯¹è¯ç³»ç»Ÿè½¬å‘LLMsï¼Œè€Œä¸”æˆ‘è®¤ä¸ºLLMsæœ€æœ‰æ½œåŠ›å®ç°æˆ‘é«˜ä¸­æ—¶è®¾æƒ³çš„AIã€‚

## æœªæ¥çš„è®¾æƒ³
LLMs + Memory + Reinforcement learning + Multimodal = AGI
 


